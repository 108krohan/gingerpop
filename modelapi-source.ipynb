{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "/home/deepak/Desktop/weaklearner\n",
      "('GET FEATURES X, Y, N_Ys', (1729, 64, 64, 1), (1729, 76), (1729, 3))\n"
     ]
    }
   ],
   "source": [
    "#vars\n",
    "xs = []\n",
    "ys = []\n",
    "n_ys = []\n",
    "\n",
    "unicode = \"2306 2310 2311 2312 2313 2315 2318 2319 2320 2322 2325 2327 2328 2330 2331 2332 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2346 2347 2348 2349 2350 2351 2352 2354 2357 2358 2359 2360 2361 2362 2363 2364 2366 2367 2368 2369 2370 2375 2376 2379 2380 2381 2382 2387 2388 2390 2392 2399 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2423 2424 2429\".split(' ')\n",
    "unicode = [int(a) for a in unicode] #convert to int\n",
    "from collections import defaultdict\n",
    "unicode2idx = defaultdict(int)\n",
    "\n",
    "idx = 0 \n",
    "for uni in unicode : \n",
    "    unicode2idx[uni] = idx\n",
    "    idx += 1\n",
    "\n",
    "idx2unicode = defaultdict(int)\n",
    "for i in range(idx) :\n",
    "    idx2unicode[i] = unicode[i]\n",
    "# print(\"idx2unicode[1] \", idx2unicode[1])\n",
    "\n",
    "def get_features(path='./clean') : \n",
    "    \n",
    "    curd = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    \n",
    "    photos =[filename for filename in os.listdir('.') if filename.endswith(\".png\")]\n",
    "    \n",
    "    global xs, ys, n_ys\n",
    "    n_labels_list = []\n",
    "    for filename in photos : \n",
    "        image = cv2.imread(filename, 0)\n",
    "        #resize my image to 64 x 64, known to be squares so aspect ratio\n",
    "        xs.append(image)\n",
    "        \n",
    "        #ys\n",
    "        labels = filename.split('_')[3:]\n",
    "        labels[-1] = labels[-1].split('.')[0] #remove .extension\n",
    "        labels = [int(label) for label in labels]\n",
    "        \n",
    "        hot_labels = [int(uni in labels) for uni in unicode]\n",
    "        ys.append(hot_labels)\n",
    "        #n_ys\n",
    "        n_labels = len(labels)\n",
    "        hot_n_labels = [int(i == n_labels) for i in range(1, 4)]\n",
    "        if sum(hot_n_labels) == 0 : \n",
    "            hot_n_labels[2] = 1\n",
    "        n_ys.append(hot_n_labels)\n",
    "\n",
    "    #test\n",
    "    \n",
    "#     result = ''\n",
    "#     one_y = ys[0]\n",
    "#     for idx in range(len(one_y)) : \n",
    "#         if one_y[idx] == 1 : \n",
    "#             result += str(idx2unicode[idx]) + ' '\n",
    "#     print(result)\n",
    "#     print(sum(ys[0]), \" == \", n_ys[0])\n",
    "#     print(photos[0])\n",
    "    \n",
    "    #conv lst to ndarray\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    n_ys = np.array(n_ys)\n",
    "    \n",
    "    #optimise and reshape for tensorflow\n",
    "    xs = xs.astype('float32')\n",
    "    xs /= 255\n",
    "    xs = xs.reshape(xs.shape[0], 64, 64, 1)\n",
    "    \n",
    "    print(\"GET FEATURES X, Y, N_Ys\", xs.shape, ys.shape, n_ys.shape)\n",
    "    os.chdir(curd)\n",
    "    return \n",
    "\n",
    "#os.chdir('/home/ashutosh/Desktop/rohan/prom')\n",
    "#os.chdir('C:\\\\Users\\\\mehul\\\\Desktop\\\\weaklearner')\n",
    "# print(os.chdir('..'))\n",
    "# print(os.getcwd())\n",
    "get_features('./clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aw/Desktop/weaklearner'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "#helper test\n",
    "print(len(unicode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [0 1 0]\n",
      "2 [0 1 0]\n",
      "1 [1 0 0]\n",
      "2 [0 1 0]\n",
      "2 [0 1 0]\n",
      "2 [0 1 0]\n",
      "1 [1 0 0]\n",
      "1 [1 0 0]\n",
      "1 [1 0 0]\n",
      "1 [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#hotcoding check \n",
    "for i in range(10) : \n",
    "    print(sum(ys[i]), n_ys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbatches = 16\n",
    "nepochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1556 samples, validate on 173 samples\n",
      "Epoch 1/45\n",
      "1556/1556 [==============================] - 6s 4ms/step - loss: 0.4221 - val_loss: 0.2498\n",
      "Epoch 2/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.1374 - val_loss: 0.1526\n",
      "Epoch 3/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0897 - val_loss: 0.1149\n",
      "Epoch 4/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0713 - val_loss: 0.0999\n",
      "Epoch 5/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0585 - val_loss: 0.0983\n",
      "Epoch 6/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0497 - val_loss: 0.0970\n",
      "Epoch 7/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0416 - val_loss: 0.1002\n",
      "Epoch 8/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0365 - val_loss: 0.0984\n",
      "Epoch 9/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.0813\n",
      "Epoch 10/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0275 - val_loss: 0.0840\n",
      "Epoch 11/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0244 - val_loss: 0.0686\n",
      "Epoch 12/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0214 - val_loss: 0.0593\n",
      "Epoch 13/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0198 - val_loss: 0.0491\n",
      "Epoch 14/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0177 - val_loss: 0.0488\n",
      "Epoch 15/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0156 - val_loss: 0.0460\n",
      "Epoch 16/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0147 - val_loss: 0.0472\n",
      "Epoch 17/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0132 - val_loss: 0.0494\n",
      "Epoch 18/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0128 - val_loss: 0.0483\n",
      "Epoch 19/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0117 - val_loss: 0.0514\n",
      "Epoch 20/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0102 - val_loss: 0.0499\n",
      "Epoch 21/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0102 - val_loss: 0.0483\n",
      "Epoch 22/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0090 - val_loss: 0.0498\n",
      "Epoch 23/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0091 - val_loss: 0.0490\n",
      "Epoch 24/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0084 - val_loss: 0.0557\n",
      "Epoch 25/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0082 - val_loss: 0.0538\n",
      "Epoch 26/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0077 - val_loss: 0.0541\n",
      "Epoch 27/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0072 - val_loss: 0.0478\n",
      "Epoch 28/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0068 - val_loss: 0.0522\n",
      "Epoch 29/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0070 - val_loss: 0.0561\n",
      "Epoch 30/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0066 - val_loss: 0.0530\n",
      "Epoch 31/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0063 - val_loss: 0.0576\n",
      "Epoch 32/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0062 - val_loss: 0.0572\n",
      "Epoch 33/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0060 - val_loss: 0.0543\n",
      "Epoch 34/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0061 - val_loss: 0.0566\n",
      "Epoch 35/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0054 - val_loss: 0.0542\n",
      "Epoch 36/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0055 - val_loss: 0.0606\n",
      "Epoch 37/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0056 - val_loss: 0.0588\n",
      "Epoch 38/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0053 - val_loss: 0.0641\n",
      "Epoch 39/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0049 - val_loss: 0.0587\n",
      "Epoch 40/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0046 - val_loss: 0.0571\n",
      "Epoch 41/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0047 - val_loss: 0.0578\n",
      "Epoch 42/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0047 - val_loss: 0.0558\n",
      "Epoch 43/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0046 - val_loss: 0.0608\n",
      "Epoch 44/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0044 - val_loss: 0.0660\n",
      "Epoch 45/45\n",
      "1556/1556 [==============================] - 4s 3ms/step - loss: 0.0046 - val_loss: 0.0584\n"
     ]
    }
   ],
   "source": [
    "def train_model(xs, ys) :\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', input_shape=(64, 64, 1)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides=2))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #dense\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(ys.shape[1], activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam())   \n",
    "    model.fit(xs, ys, batch_size = 32, shuffle = True, epochs = 45, verbose = 1, validation_split = 0.10)    \n",
    "    return model\n",
    "\n",
    "#test\n",
    "# print(xs.shape, type(xs))\n",
    "# print(ys.shape, type(ys))\n",
    "model = train_model(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"model\"\n",
    "n_model_name = \"n_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def save_model(model, name) : \n",
    "    model_json = model.to_json()\n",
    "    with open(name + \".json\", \"w\") as json_file : \n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(name + \".h5\")\n",
    "    print(\"saved {} to disk\".format(name))\n",
    "    \n",
    "#save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model to disk\n"
     ]
    }
   ],
   "source": [
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_name) : \n",
    "    json_file = open(model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(model_name + \".h5\")\n",
    "    print(\"Loaded {} from disk\".format(model_name))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model_name = \"model\"\n",
    "n_model_name = \"n_model\"\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1556 samples, validate on 173 samples\n",
      "Epoch 1/20\n",
      "1556/1556 [==============================] - 6s 4ms/step - loss: 0.8389 - acc: 0.7346 - val_loss: 3.2745 - val_acc: 0.4104\n",
      "Epoch 2/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.4396 - acc: 0.8490 - val_loss: 4.6396 - val_acc: 0.4104\n",
      "Epoch 3/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.2919 - acc: 0.9004 - val_loss: 5.7811 - val_acc: 0.4104\n",
      "Epoch 4/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.2062 - acc: 0.9222 - val_loss: 6.0916 - val_acc: 0.4104\n",
      "Epoch 5/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.1721 - acc: 0.9389 - val_loss: 6.0076 - val_acc: 0.4104\n",
      "Epoch 6/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.1462 - acc: 0.9524 - val_loss: 4.9068 - val_acc: 0.4104\n",
      "Epoch 7/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.1453 - acc: 0.9518 - val_loss: 3.8439 - val_acc: 0.4104\n",
      "Epoch 8/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.1241 - acc: 0.9550 - val_loss: 4.0245 - val_acc: 0.4104\n",
      "Epoch 9/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0927 - acc: 0.9627 - val_loss: 2.9032 - val_acc: 0.4277\n",
      "Epoch 10/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0812 - acc: 0.9685 - val_loss: 1.6899 - val_acc: 0.5665\n",
      "Epoch 11/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0743 - acc: 0.9756 - val_loss: 0.6597 - val_acc: 0.7630\n",
      "Epoch 12/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0438 - acc: 0.9871 - val_loss: 0.4563 - val_acc: 0.8613\n",
      "Epoch 13/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0467 - acc: 0.9852 - val_loss: 0.5156 - val_acc: 0.8439\n",
      "Epoch 14/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0329 - acc: 0.9910 - val_loss: 0.4708 - val_acc: 0.8613\n",
      "Epoch 15/20\n",
      "1556/1556 [==============================] - 5s 4ms/step - loss: 0.0300 - acc: 0.9929 - val_loss: 0.6267 - val_acc: 0.8266\n",
      "Epoch 16/20\n",
      "1556/1556 [==============================] - 5s 4ms/step - loss: 0.0308 - acc: 0.9897 - val_loss: 0.4582 - val_acc: 0.8844\n",
      "Epoch 17/20\n",
      "1556/1556 [==============================] - 5s 3ms/step - loss: 0.0273 - acc: 0.9897 - val_loss: 0.4699 - val_acc: 0.8728\n",
      "Epoch 18/20\n",
      "1556/1556 [==============================] - 5s 4ms/step - loss: 0.0319 - acc: 0.9916 - val_loss: 0.4866 - val_acc: 0.8902\n",
      "Epoch 19/20\n",
      "1556/1556 [==============================] - 6s 4ms/step - loss: 0.0263 - acc: 0.9916 - val_loss: 0.5162 - val_acc: 0.8728\n",
      "Epoch 20/20\n",
      "1556/1556 [==============================] - 7s 4ms/step - loss: 0.0260 - acc: 0.9936 - val_loss: 0.5671 - val_acc: 0.8902\n",
      "saved n_model to disk\n"
     ]
    }
   ],
   "source": [
    "def train_n_model(xs, n_ys) :\n",
    "    global batch_size, epochs\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 1)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #dense\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_ys.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics = ['accuracy'])   \n",
    "    model.fit(xs, n_ys, batch_size = 32, shuffle = True, epochs = 20, verbose = 1, validation_split = 0.10)    \n",
    "    return model\n",
    "\n",
    "n_model = train_n_model(xs, n_ys)\n",
    "save_model(n_model, n_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_name)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded n_model from disk\n"
     ]
    }
   ],
   "source": [
    "n_model = load_model(n_model_name)\n",
    "# n_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(imagename) : \n",
    "    image = cv2.imread(imagename, 0)\n",
    "    bilateral = cv2.bilateralFilter(image, 0, 15, 23)\n",
    "    median = cv2.medianBlur(bilateral, 5)\n",
    "    _, otsu = cv2.threshold(median, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    #padding to 150, 150, [96, x]\n",
    "    #image.shape[0] = height\n",
    "    #image.shape[1] = width\n",
    "    rows, cols = image.shape[0:2]\n",
    "\n",
    "    #difference to pad up to get square image\n",
    "    diff = abs(rows - cols)\n",
    "    one = diff // 2\n",
    "    other = diff // 2    #integer division\n",
    "    if diff % 2 != 0 :   #131 - 128 square // squaring k liye\n",
    "        other += 1\n",
    "\n",
    "    if rows > cols : \n",
    "        pad = cv2.copyMakeBorder(otsu, 0, 0, one, other, cv2.BORDER_CONSTANT, 0)\n",
    "    else : \n",
    "        pad = cv2.copyMakeBorder(otsu, one, other, 0, 0, cv2.BORDER_CONSTANT, 0)\n",
    "\n",
    "    #bring to uniform size\n",
    "    size = max(rows, cols)\n",
    "\n",
    "    if size >= 64 : \n",
    "        clean = cv2.resize(pad, (64, 64))\n",
    "    else : \n",
    "        pixel_pad = 64 - size\n",
    "        one = pixel_pad // 2\n",
    "        other = pixel_pad // 2\n",
    "        if pixel_pad % 2 == 1 : \n",
    "            other += 1\n",
    "        clean = cv2.copyMakeBorder(pad, one, other, one, other, cv2.BORDER_CONSTANT, 0)\n",
    "    return clean\n",
    "\n",
    "def predict(imagename) : \n",
    "    global model, n_model\n",
    "    global idx2unicode, unicode2idx, unicode\n",
    "    clean = process(imagename)\n",
    "    clean = clean.reshape(1, 64, 64, 1)\n",
    "    clean = clean.astype('float32')\n",
    "    clean /= 255\n",
    "    \n",
    "    n_probs = n_model.predict(clean)[0]\n",
    "    n_predictions = np.argmax(n_probs) + 1\n",
    "    \n",
    "    predictions = model.predict(clean)[0]\n",
    "    top_n = np.argsort(predictions)[-n_predictions:] #list of toppers (reversed)\n",
    "    \n",
    "#     print(result)\n",
    "    result = []\n",
    "    #convert idx to unicode \n",
    "    for i in top_n : \n",
    "        result.append(idx2unicode[i])\n",
    "    result.reverse()\n",
    "    \n",
    "    #test\n",
    "    print(imagename)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = \"/home/aw/Desktop/prom/test/page1_15_0_2350_2375.png\"\n",
    "# predict(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1729\n",
      "page0_0_10_2313.png\n",
      "page0_0_10_2313.png [2346, 2306]\n",
      "page0_0_11_2352.png\n",
      "page0_0_11_2352.png [2360, 2346]\n",
      "page0_0_13_2340_2367.png\n",
      "page0_0_13_2340_2367.png [2306, 2346]\n",
      "page0_0_15_2312.png\n",
      "page0_0_15_2312.png [2343, 2368]\n",
      "page0_0_16_2344.png\n",
      "page0_0_16_2344.png [2366, 2332, 2342]\n",
      "page0_0_1_2361_2380.png\n",
      "page0_0_1_2361_2380.png [2332, 2306]\n",
      "page0_0_2_2312.png\n",
      "page0_0_2_2312.png [2360]\n",
      "page0_0_3_2344.png\n",
      "page0_0_3_2344.png [2342, 2366]\n",
      "page0_0_5_2346_2368.png\n",
      "page0_0_5_2346_2368.png [2340, 2354]\n",
      "page0_0_6_2325_2368.png\n",
      "page0_0_6_2325_2368.png [2306, 2366]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.chdir('C:\\\\Users\\\\mehul\\\\Desktop\\\\weaklearner\\\\clean\\\\')\n",
    "\n",
    "photos = [img for img in os.listdir('.') if img.endswith('.png')]\n",
    "print(len(photos))\n",
    "sub = photos[0:10]\n",
    "for e in sub : \n",
    "    print(e, predict(e))\n",
    "#predict('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
